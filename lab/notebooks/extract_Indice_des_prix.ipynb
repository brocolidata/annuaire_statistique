{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract & Load *Indice des prix* \n",
    "\n",
    "**TODO** : \n",
    "- Fix offset from years and dimensions merge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import dictconfig\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_NAME = \"indice_des_prix\"\n",
    "SOURCE_SETTINGS_PATH = pathlib.Path('/dataplatform_lab', 'lab', 'notebooks', 'sources')\n",
    "DATA_PATH = pathlib.Path('/dataplatform_lab', 'lab', 'dwh_data')\n",
    "EXTRACTS_PATH = pathlib.Path(DATA_PATH, 'extracts')\n",
    "LOAD_PATH = pathlib.Path(DATA_PATH, 'raw')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config() -> dictconfig.DictConfig:\n",
    "    config_path = SOURCE_SETTINGS_PATH.relative_to(os.getcwd()).as_posix()\n",
    "    with initialize(\n",
    "        version_base=None, \n",
    "        config_path=config_path\n",
    "    ):\n",
    "        return compose(config_name=CONFIG_NAME)\n",
    "\n",
    "\n",
    "def get_available_year(historic_years, year):\n",
    "    return list(range(year-1, year-(historic_years+1), -1))\n",
    "\n",
    "\n",
    "def get_years_to_load(historic_years, years):\n",
    "    years.sort(reverse=True)\n",
    "    dc_available_years = {\n",
    "        y:get_available_year(historic_years, y) for y in years\n",
    "    }\n",
    "    dc_years = {}\n",
    "    for year in years:\n",
    "        dc = {\n",
    "            ay:year for ay in dc_available_years[year] if ay not in dc_years\n",
    "        }\n",
    "        dc_years.update(dc)\n",
    "    return dc_years\n",
    "\n",
    "\n",
    "def get_excel_file(folder_path: pathlib.Path, excel_file_name: str) -> pd.ExcelFile:\n",
    "    excel_file_path = pathlib.Path(folder_path, excel_file_name)\n",
    "    return pd.ExcelFile(excel_file_path)\n",
    "\n",
    "\n",
    "def process_column_names(\n",
    "    df,\n",
    "    dimension_cols = None,\n",
    "    dimension_cols_rename = None\n",
    "):\n",
    "    rename_dc = {}\n",
    "    value_cols = df.columns.difference(dimension_cols)\n",
    "    if dimension_cols_rename:\n",
    "        rename_dc.update(\n",
    "            {\n",
    "                old:new for old, new in zip(\n",
    "                    dimension_cols, dimension_cols_rename\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    for col in value_cols:\n",
    "        renamed_col = ' '.join([x for x in re.findall(r\"[^\\W]*\",col) if x])\n",
    "        rename_dc.update({col: renamed_col})\n",
    "\n",
    "    return (\n",
    "        df\n",
    "        .rename(columns=rename_dc)\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_multiline_labels(\n",
    "        df: pd.DataFrame,\n",
    "        dimension_cols: list\n",
    "    ) -> pd.DataFrame:\n",
    "    col_years=list(df.columns.difference(dimension_cols))\n",
    "    ls_produits = []\n",
    "    previous = {}\n",
    "    for i, row in df.iterrows():\n",
    "        row_dc = row.to_dict()\n",
    "        actual = {col:row_dc[col] for col in dimension_cols}\n",
    "        elem = row_dc[col_years[0]]\n",
    "        if pd.isna(elem):\n",
    "            previous = actual\n",
    "        elif previous:\n",
    "            ls_produits.append(\n",
    "                {\n",
    "                    col:previous.get(col).strip() + \"\" + actual.get(col).strip() for col in dimension_cols\n",
    "                }\n",
    "            )\n",
    "            previous = {}\n",
    "        else:\n",
    "            ls_produits.append(\n",
    "                {\n",
    "                    col:actual.get(col).strip() for col in dimension_cols\n",
    "                }\n",
    "            )\n",
    "            previous = \"\"\n",
    "\n",
    "    df_produits = pd.DataFrame(ls_produits)\n",
    "    \n",
    "    # clean values\n",
    "    df_values = (\n",
    "        df[col_years]\n",
    "        .dropna(\n",
    "            axis=0,\n",
    "            how='any',\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    return pd.concat([df_produits, df_values], axis=1)\n",
    "\n",
    "\n",
    "def prepare_df(version, df:pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    dimension_cols = df.columns[version.config.dimension_cols_index]\n",
    "    clean_col_df = process_column_names(\n",
    "        df, \n",
    "        dimension_cols=dimension_cols,\n",
    "        dimension_cols_rename=version.config.dimension_cols_rename\n",
    "    )\n",
    "    dimension_cols = clean_col_df.columns[version.config.dimension_cols_index]\n",
    "    clean_df = merge_multiline_labels(clean_col_df, dimension_cols) \n",
    "\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def save_df(df, version):\n",
    "    path = pathlib.Path(\n",
    "        LOAD_PATH,\n",
    "        version.dataset.source.schema,\n",
    "        f\"{version.fqtn}.parquet\"\n",
    "    )\n",
    "    if not path.parent.exists():\n",
    "        path.parent.mkdir(parents=True)\n",
    "    \n",
    "    df.to_parquet(\n",
    "        path,\n",
    "        index = None\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetVersion:\n",
    "\n",
    "    def __init__(self, dataset, version_dict, config={}):\n",
    "        self.dataset = dataset\n",
    "        version_config = version_dict.get('config')\n",
    "        self.config = OmegaConf.unsafe_merge(config, version_config)\n",
    "        if not self.config_is_valid():\n",
    "            warnings.warn('The configuration is incorrect for this dataset')\n",
    "        self.version_num = version_dict[\"version\"]\n",
    "        self.historic_years = self.config.historic_years\n",
    "        self.years_dc = version_dict[\"years\"]\n",
    "        self.years = [y[\"year\"] for y in self.years_dc]\n",
    "        self.files_to_load = self.get_files_to_load_per_year()\n",
    "\n",
    "    @property\n",
    "    def fqn(self):\n",
    "        dataset_name = self.dataset.name\n",
    "        return f\"{self.dataset.name} v{self.version_num}\"\n",
    "    \n",
    "    @property\n",
    "    def fqtn(self):\n",
    "        return f\"{self.dataset.table_name}_v{self.version_num}\"\n",
    "    \n",
    "    @property\n",
    "    def files_to_load_ls(self):\n",
    "        return list(set(self.files_to_load.values()))\n",
    "    \n",
    "    @property\n",
    "    def available_years(self):\n",
    "        return list(self.files_to_load.keys())\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.fqn\n",
    "    \n",
    "\n",
    "    def get_files_to_load_per_year(self):\n",
    "        years_to_load_dc = get_years_to_load(\n",
    "            self.historic_years, \n",
    "            self.years\n",
    "        )\n",
    "        file_names_dc = self.dataset.source.file_names_dc\n",
    "        return {k:file_names_dc[v] for k,v in years_to_load_dc.items()}\n",
    "    \n",
    "    def get_years_to_load_per_file(self):\n",
    "        years_to_load_dc = get_years_to_load(\n",
    "            self.historic_years, \n",
    "            self.years\n",
    "        )\n",
    "        dc_years_per_file = {y:[] for y in years_to_load_dc.values()}\n",
    "        for col_year, file_year in years_to_load_dc.items():\n",
    "            dc_years_per_file[file_year].append(str(col_year))\n",
    "        \n",
    "        return dc_years_per_file\n",
    "\n",
    "    \n",
    "    def config_is_valid(self):\n",
    "        ls_required_keys = [\n",
    "            \"sheet_name\",\n",
    "            \"historic_years\",\n",
    "            \"type\",\n",
    "            \"base_year_cell\",\n",
    "            \"header_row\",\n",
    "            \"data_start_row\",\n",
    "            \"data_end_row\"\n",
    "        ]\n",
    "        for key in ls_required_keys:\n",
    "            if key not in self.config:\n",
    "                return False\n",
    "\n",
    "        if (\n",
    "            self.dataset.type_data == \"index\" and\n",
    "            \"base_year_cell\" not in self.config\n",
    "        ):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "            \n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, source, dataset_dict):\n",
    "        self.source = source\n",
    "        self.name = dataset_dict.name\n",
    "        self.table_name = dataset_dict.table_name\n",
    "        self.config = dataset_dict.get('config', {})\n",
    "        self.type_data = self.config.get('type')\n",
    "        self.versions_ls = dataset_dict[\"versions\"]\n",
    "        self.versions = []\n",
    "        self.process_versions()\n",
    "\n",
    "\n",
    "    def process_versions(self):\n",
    "        for version_dict in self.versions_ls:\n",
    "            version = DatasetVersion(self, version_dict, self.config)\n",
    "            self.versions.append(version)\n",
    "            \n",
    "\n",
    "\n",
    "class Source:\n",
    "\n",
    "    def __init__(self, source_config):\n",
    "        self.name = source_config.name\n",
    "        self.file_names_dc = source_config.file_names\n",
    "        self.schema = source_config.schema\n",
    "        self.datasets_ls = source_config.datasets\n",
    "        self.datasets = []\n",
    "        self.process_datasets()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "    \n",
    "    def process_datasets(self):\n",
    "        for dataset_dict in self.datasets_ls:\n",
    "            dataset = Dataset(self, dataset_dict)\n",
    "            self.datasets.append(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "ipp_source = Source(config)\n",
    "dataset = ipp_source.datasets[0]\n",
    "version = dataset.versions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_df = {}\n",
    "dc_years_file = version.get_years_to_load_per_file()\n",
    "for file_name in version.files_to_load_ls:\n",
    "    file_year = re.findall(r\"\\d{4}\", file_name)[-1]\n",
    "    folder_path = pathlib.Path(EXTRACTS_PATH, f\"Annuaire Statistique {file_year}\")\n",
    "    excel_file = get_excel_file(folder_path, file_name)\n",
    "    df = pd.read_excel(\n",
    "        excel_file,\n",
    "        version.config.sheet_name,\n",
    "        header=0,\n",
    "        skiprows=version.config.skiprows,\n",
    "        skipfooter=version.config.skipfooter\n",
    "    )\n",
    "    prep_df = prepare_df(version, df)\n",
    "    keep_cols = dc_years_file[int(file_year)]\n",
    "    dc_df[file_year] = prep_df[keep_cols]\n",
    "    dimensions_df = prep_df[version.config.dimension_cols_rename]\n",
    "\n",
    "\n",
    "ls_dfs = [dimensions_df, *[df for df in dc_df.values()]]\n",
    "\n",
    "df = pd.concat(ls_dfs,  axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libelle_fr\n",
      "libelle_ar\n",
      "2016\n",
      "2019\n",
      "2018\n",
      "2017\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load to DuckDB raw database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "%load_ext sql\n",
    "\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect(f\"{os.getenv('DUCKDB_DATA')}/raw_duck.db\")\n",
    "%sql conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/dataplatform_lab/lab/dwh_data/raw/indice_des_prix/ipp_v1.parquet')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = pathlib.Path(\n",
    "    LOAD_PATH,\n",
    "    version.dataset.source.schema,\n",
    "    f\"{version.fqtn}.parquet\"\n",
    ")\n",
    "\n",
    "if not load_path.parent.exists():\n",
    "    load_path.parent.mkdir(parents=True)\n",
    "\n",
    "load_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count\n",
       "0     31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql COPY df TO '{{ load_path.as_posix() }}' (FORMAT PARQUET);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
